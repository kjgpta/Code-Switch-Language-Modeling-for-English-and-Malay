{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Get Parser based statistics\n"
      ],
      "metadata": {
        "id": "wBT2r4l_Scuo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63PNGMIDZX_P"
      },
      "outputs": [],
      "source": [
        "!pip -q install jedi==0.10\n",
        "!pip -q install torch==1.12.1\n",
        "!pip -q install -U pip setuptools wheel\n",
        "!pip -q install -U spacy\n",
        "!pip -q install wget\n",
        "!pip -q install benepar\n",
        "!pip -q install deep-translator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import benepar\n",
        "import svgling\n",
        "from pathlib import Path\n",
        "import en_core_web_sm\n",
        "import nltk\n",
        "import pickle as pkl\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "eng_dep = en_core_web_sm.load()"
      ],
      "metadata": {
        "id": "6kov-maeZY5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_analyzer(word, tag):\n",
        "    ar = \"\"\n",
        "    tag2= tag\n",
        "    if tag == \"<malay>\":\n",
        "        tag2 = \"</malay>\"\n",
        "    if tag == \"(\":\n",
        "        tag2 = \")\"\n",
        "    if tag == \"[\":\n",
        "        tag2 = \"]\"\n",
        "    if tag == \"<\":\n",
        "        tag2 = \">\"\n",
        "    if tag == \"{\":\n",
        "        tag2 = \"}\"\n",
        "    n = len(tag)\n",
        "    k = len(tag2)\n",
        "    if word.startswith(tag) and word.endswith(tag2):\n",
        "            ar = word[n:-k]\n",
        "    elif word.startswith(tag):\n",
        "            ar = word[n:]\n",
        "    elif word.endswith(tag2):\n",
        "            ar = word[:-k]\n",
        "    return ar"
      ],
      "metadata": {
        "id": "mbE0f_m3Zk-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_cleanser(string):\n",
        "    words = string.split()\n",
        "    eng_ar = []\n",
        "    mal_ar = []\n",
        "    flag = False\n",
        "    flag2 = False\n",
        "    eng = \"\"\n",
        "    mal = \"\"\n",
        "    ar = []\n",
        "    sent = \"\"\n",
        "    for i in range(len(words)):\n",
        "        word = words[i]\n",
        "        if i == 0 and word.startswith(\"<malay>\"):\n",
        "            flag = True\n",
        "        if \"_\" in word:\n",
        "            wrd = \"\"\n",
        "            for j in range(len(word)):\n",
        "                if word[j] != \"_\":\n",
        "                    wrd += word[j]\n",
        "            word = wrd\n",
        "            if flag2:\n",
        "                mal += word+ \" \"\n",
        "            else:\n",
        "                eng += word+ \" \"\n",
        "        elif word.startswith(\"<malay>\") or word.endswith(\"</malay>\"):\n",
        "            word2 = word\n",
        "            if word.startswith(\"<malay>\"):\n",
        "                flag2 = True\n",
        "                eng_ar.append(eng)\n",
        "                eng = \"\"\n",
        "            word = word_analyzer(word, \"<malay>\")\n",
        "            mal += word+ \" \"\n",
        "            if word2.endswith(\"</malay>\"):\n",
        "                flag2 = False\n",
        "                mal_ar.append(mal)\n",
        "                mal = \"\"\n",
        "        elif word.startswith(\"(\") or word.endswith(\")\"):\n",
        "            word = word_analyzer(word, \"(\")\n",
        "            if flag2:\n",
        "                mal += word+ \" \"\n",
        "            else:\n",
        "                eng += word+ \" \"\n",
        "        elif word.startswith(\"[\") or word.endswith(\"]\"):\n",
        "            word = word_analyzer(word, \"[\")\n",
        "            if flag2:\n",
        "                mal += word+ \" \"\n",
        "            else:\n",
        "                eng += word+ \" \"\n",
        "        elif word.startswith(\"<\") or word.endswith(\">\"):\n",
        "            word = word_analyzer(word, \"<\")\n",
        "            if flag2:\n",
        "                mal += word+ \" \"\n",
        "            else:\n",
        "                eng += word+ \" \"\n",
        "        elif word.startswith(\"{\") or word.endswith(\"}\"):\n",
        "            word = word_analyzer(word, \"{\")\n",
        "            if flag2:\n",
        "                mal += word+ \" \"\n",
        "            else:\n",
        "                eng += word+ \" \"\n",
        "        elif word.startswith(\"!\") or word.endswith(\"!\"):\n",
        "            word = word_analyzer(word, \"!\")\n",
        "            if flag2:\n",
        "                mal += word+ \" \"\n",
        "            else:\n",
        "                eng += word+ \" \"\n",
        "        elif word.startswith(\"#\") or word.endswith(\"#\"):\n",
        "            word = word_analyzer(word, \"#\")\n",
        "            if flag2:\n",
        "                mal += word+ \" \"\n",
        "            else:\n",
        "                eng += word+ \" \"\n",
        "        elif word.endswith(\"~\"):\n",
        "            continue\n",
        "        else:\n",
        "            if flag2:\n",
        "                mal += word+ \" \"\n",
        "            else:\n",
        "                eng += word+ \" \"\n",
        "    if mal == \"\" and eng != \"\":\n",
        "        eng_ar.append(eng)\n",
        "    elif mal != \"\" and eng == \"\":\n",
        "        mal_ar.append(mal)\n",
        "    return eng_ar, mal_ar, flag"
      ],
      "metadata": {
        "id": "ZB-KvZXCZmh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_array(mal_ar):\n",
        "    eng_ar = []\n",
        "    for data in mal_ar:\n",
        "        eng_ar.append(GoogleTranslator(source='ms', target='en').translate(data[:-1]))\n",
        "    return eng_ar"
      ],
      "metadata": {
        "id": "r_GilWCAZoIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_parser(txt, eng_ar, mal_eng_ar, flag):\n",
        "    i = 0\n",
        "    j = 0\n",
        "    if flag:\n",
        "        while i<len(eng_ar) or j<len(mal_eng_ar):\n",
        "            if j<len(mal_eng_ar):\n",
        "                if txt in mal_eng_ar[j] or txt in lemmatizer.lemmatize(mal_eng_ar[j]):\n",
        "                    return True\n",
        "                j += 1\n",
        "            if i<len(eng_ar):\n",
        "                if txt in eng_ar[i] or txt in lemmatizer.lemmatize(eng_ar[i]):\n",
        "                    return False\n",
        "                i += 1\n",
        "        return False\n",
        "    else:\n",
        "        while i<len(eng_ar) or j<len(mal_eng_ar):\n",
        "            if i<len(eng_ar):\n",
        "                if txt in eng_ar[i] or txt in lemmatizer.lemmatize(eng_ar[i]):\n",
        "                    return False\n",
        "                i += 1\n",
        "            if j<len(mal_eng_ar):\n",
        "                if txt in mal_eng_ar[j] or txt in lemmatizer.lemmatize(mal_eng_ar[j]):\n",
        "                    return True\n",
        "                j += 1\n",
        "        return False"
      ],
      "metadata": {
        "id": "N_P8FvVZZ4hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"CS_Data.txt\"\n",
        "with open(filename) as f:\n",
        "    cs_data = f.readlines()\n",
        "\n",
        "filename2 = \"Eng_CS_final_Data.txt\" # Translated file of code-switched data using Deep-Translator\n",
        "with open(filename2) as f2:\n",
        "    eng_cs_data = f2.readlines()"
      ],
      "metadata": {
        "id": "lFeqRrNTZ5rJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ar_eng_parser = dict()\n",
        "ar_mal_parser = dict()\n",
        "exc = []\n",
        "\n",
        "for i in range(len(cs_data)):\n",
        "    try:  \n",
        "        cs_sent = cs_data[i]\n",
        "        eng_cs_sent = eng_cs_data[i]\n",
        "        eng_ar, mal_ar, flag = data_cleanser(cs_sent)\n",
        "        mal_eng_ar = translate_array(mal_ar)\n",
        "        sent = \"\"\n",
        "        if flag:\n",
        "            i = 0\n",
        "            j = 0\n",
        "            while i<len(mal_eng_ar) or j < len(eng_ar):\n",
        "                if i<len(mal_eng_ar):\n",
        "                    sent += mal_eng_ar[i] + \" \"\n",
        "                    i += 1\n",
        "                if j < len(eng_ar):\n",
        "                    sent += eng_ar[j]\n",
        "                    j += 1\n",
        "        else:\n",
        "            i = 0\n",
        "            j = 0\n",
        "            while i<len(mal_eng_ar) or j < len(eng_ar):\n",
        "                if j < len(eng_ar):\n",
        "                    sent += eng_ar[j]\n",
        "                    j += 1\n",
        "                if i<len(mal_eng_ar):\n",
        "                    sent += mal_eng_ar[i] +\" \"\n",
        "                    i += 1\n",
        "        \n",
        "        doc = eng_dep(eng_cs_sent)\n",
        "        for token in doc:\n",
        "            dep = token.dep_\n",
        "            fg = check_parser(token.text, eng_ar, mal_eng_ar, flag)\n",
        "            if fg:\n",
        "                if dep not in ar_mal_parser.keys():\n",
        "                    ar_mal_parser[dep] = 1\n",
        "                else:\n",
        "                    ar_mal_parser[dep] += 1\n",
        "            else:\n",
        "                if dep not in ar_eng_parser.keys():\n",
        "                    ar_eng_parser[dep] = 1\n",
        "                else:\n",
        "                    ar_eng_parser[dep] += 1\n",
        "    except:\n",
        "        exc.append(i)"
      ],
      "metadata": {
        "id": "rhkNz2teZ-IV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_parser_tags = [\"ROOT\", \"acl\", \"acomp\", \"advcl\", \"advmod\", \"agent\", \"amod\", \"appos\", \"attr\", \"aux\", \"auxpass\", \"case\", \"cc\", \"ccomp\", \"compound\", \"conj\", \"csubj\", \"csubjpass\", \"dative\", \"dep\", \"det\", \"dobj\", \"expl\", \"intj\", \"mark\", \"meta\", \"neg\", \"nmod\", \"npadvmod\", \"nsubj\", \"nsubjpass\", \"nummod\", \"oprd\", \"parataxis\", \"pcomp\", \"pobj\", \"poss\", \"preconj\", \"predet\", \"prep\", \"prt\", \"punct\", \"quantmod\", \"relcl\", \"xcomp\"]"
      ],
      "metadata": {
        "id": "Vf8PnIGlaFF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(column = [\"Parser Tag\", \"# English Instance\", \"# Malay Instances\", \"Malay Percentage\"])\n",
        "for dep in list_of_parser_tags:\n",
        "    try: \n",
        "        eng = ar_eng_parser[dep]\n",
        "    except: \n",
        "        eng = 0\n",
        "    try: \n",
        "        mal = ar_mal_parser[dep]\n",
        "    except:\n",
        "        mal = 0\n",
        "    if eng+mal != 0:\n",
        "        df.loc[len(df.index)] = [dep, eng, mal, round(mal*100/(eng+mal), 2)]\n",
        "\n",
        "df.to_excel(\"Parser_results.xlsx\", index = False)"
      ],
      "metadata": {
        "id": "UGHjmwIXQZCx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}